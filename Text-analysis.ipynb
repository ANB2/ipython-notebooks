{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Twitter text\n",
      "\n",
      "Let's load one day's worth of tweets from India. These were\n",
      "[captured](https://github.com/gramener/twitter-stream) via the\n",
      "[Twitter API](https://dev.twitter.com/). The file is at <http://files.gramener.com/data/tweets.20130919.json.gz>.\n",
      "It's just under 7MB.\n",
      "\n",
      "First, let's download the file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import urllib\n",
      "\n",
      "tweetfile = 'tweets.json.gz'\n",
      "if not os.path.exists(tweetfile):\n",
      "    url = 'http://files.gramener.com/data/tweets.20130919.json.gz'\n",
      "    urllib.urlretrieve(url, tweetfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This file is not *quite* a gzipped JSON file, despite the file name. Each row is a JSON string. Some lines might be blank -- especially alternate lines."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gzip\n",
      "for line in gzip.open(tweetfile).readlines()[:8]:\n",
      "    print line[:60]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\"created_at\":\"Wed Sep 18 03:39:02 +0000 2013\",\"id\":38017409\n",
        "\n",
        "\n",
        "{\"created_at\":\"Wed Sep 18 03:39:02 +0000 2013\",\"id\":38017409\n",
        "\n",
        "\n",
        "{\"created_at\":\"Wed Sep 18 03:39:06 +0000 2013\",\"id\":38017411\n",
        "\n",
        "\n",
        "{\"created_at\":\"Wed Sep 18 03:39:16 +0000 2013\",\"id\":38017415\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's load this into a Pandas data structure. After some experimentation, I find that this is a reasonably fast way of loading it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "series = pd.Series([\n",
      "    line for line in gzip.open(tweetfile) if line.strip()\n",
      "]).apply(json.loads)\n",
      "\n",
      "data = pd.DataFrame({\n",
      "  'id'  : series.apply(lambda t: t['id_str']),\n",
      "  'name': series.apply(lambda t: t['user']['screen_name']),\n",
      "  'text': series.apply(lambda t: t['text']),\n",
      "}).set_index('id')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've extracted just 3 things from the tweets -- the ID (which we set as the index), the person who tweeted it, and the text of the tweet. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>name</th>\n",
        "      <th>text</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>id</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>380174094936702976</th>\n",
        "      <td>          rgokul</td>\n",
        "      <td> \u0baa\u0bbf\u0ba9\u0bcd\u0ba9\u0bbe\u0b9f\u0bbf \u0baa\u0bbe\u0ba4\u0bcd\u0ba4\u0bbe \u0baa\u0bb0\u0bcd\u0bb8\u0bcd\u0ba9\u0bbe\u0bb2\u0bbf\u0b9f\u0bcd\u0b9f\u0bbf, \u0bae\u0bc1\u0ba9\u0bcd\u0ba9\u0bbe\u0b9f\u0bbf \u0baa\u0bbe\u0ba4\u0bcd\u0ba4\u0bbe...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>380174096635416577</th>\n",
        "      <td>         fknadaf</td>\n",
        "      <td>                   @rehu123 \\nHi..re   h r u..????</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>380174111076405248</th>\n",
        "      <td>  neetakolhatkar</td>\n",
        "      <td> @sohamsabnis mhanunach jau dya..tyat phile jod...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>380174154751696896</th>\n",
        "      <td>       pinashah1</td>\n",
        "      <td>                         @Miragpur7 jok of tha day</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>380174182803202050</th>\n",
        "      <td> MeghaLvsShaleen</td>\n",
        "      <td> @ilovearrt @shweet_tasu @akanksha_pooh31 @Miss...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "                               name                                               text\n",
        "id                                                                                    \n",
        "380174094936702976           rgokul  \u0baa\u0bbf\u0ba9\u0bcd\u0ba9\u0bbe\u0b9f\u0bbf \u0baa\u0bbe\u0ba4\u0bcd\u0ba4\u0bbe \u0baa\u0bb0\u0bcd\u0bb8\u0bcd\u0ba9\u0bbe\u0bb2\u0bbf\u0b9f\u0bcd\u0b9f\u0bbf, \u0bae\u0bc1\u0ba9\u0bcd\u0ba9\u0bbe\u0b9f\u0bbf \u0baa\u0bbe\u0ba4\u0bcd\u0ba4\u0bbe...\n",
        "380174096635416577          fknadaf                    @rehu123 \\nHi..re   h r u..????\n",
        "380174111076405248   neetakolhatkar  @sohamsabnis mhanunach jau dya..tyat phile jod...\n",
        "380174154751696896        pinashah1                          @Miragpur7 jok of tha day\n",
        "380174182803202050  MeghaLvsShaleen  @ilovearrt @shweet_tasu @akanksha_pooh31 @Miss..."
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's do some basic text analysis on this.\n",
      "\n",
      "## Most frequent words\n",
      "\n",
      "Let's get the full text as a string and count the words. Let's assume that words are split by a single space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = pd.Series(' '.join(data['text']).split(' '))\n",
      "words.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 51,
       "text": [
        "to     3256\n",
        "the    3235\n",
        "       2441\n",
        "in     2275\n",
        "a      2193\n",
        "is     2139\n",
        "of     2079\n",
        "I      1817\n",
        "and    1567\n",
        "for    1537\n",
        "at     1497\n",
        "you    1342\n",
        "on     1053\n",
        "I'm     970\n",
        "it      939\n",
        "...\n",
        "http://t.co/necdoOUAHU     1\n",
        "Ravjiani,                  1\n",
        "#TheAsianAge               1\n",
        "coffee.!!                  1\n",
        "race's                     1\n",
        "http://t.co/aH4i8A0Nz1\"    1\n",
        "real\u2026                      1\n",
        "http://t.co/GNzghJBYX1     1\n",
        "update??                   1\n",
        "#lazy                      1\n",
        "107,#Gurgaon,              1\n",
        "annaru                     1\n",
        "snooping                   1\n",
        "@BangaloreAshram           1\n",
        "\u091c\u0948\u0928\u0964                       1\n",
        "Length: 65123, dtype: int64"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are lots of errors in the assumption that words are split by a single space. That ignores punctuation, multiple spaces, hyphenation, and a lot of other things. But **it's not a bad starting point** and you can start making reasonable inferences as a first approximation.\n",
      "\n",
      "## Remove stopwords\n",
      "\n",
      "The bigger problem is that the most common words are also the most often used -- to, the, in, a, etc. These are called **stopwords**. We need a way of finding and removing them.\n",
      "\n",
      "NLTK offers a standard list of stopwords. This is what we get if we remove those."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "ignore = set(stopwords.words('english')) & set(words.unique())\n",
      "words.value_counts().drop(ignore)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "         2441\n",
        "I        1817\n",
        "I'm       970\n",
        "u         695\n",
        "-         604\n",
        "@         507\n",
        "The       503\n",
        ":)        493\n",
        "&amp;     467\n",
        "like      390\n",
        "hai       364\n",
        "(@        363\n",
        "good      333\n",
        "one       285\n",
        "get       285\n",
        "...\n",
        "http://t.co/necdoOUAHU     1\n",
        "Ravjiani,                  1\n",
        "#TheAsianAge               1\n",
        "coffee.!!                  1\n",
        "race's                     1\n",
        "http://t.co/aH4i8A0Nz1\"    1\n",
        "real\u2026                      1\n",
        "http://t.co/GNzghJBYX1     1\n",
        "update??                   1\n",
        "#lazy                      1\n",
        "107,#Gurgaon,              1\n",
        "annaru                     1\n",
        "snooping                   1\n",
        "@BangaloreAshram           1\n",
        "\u091c\u0948\u0928\u0964                       1\n",
        "Length: 64998, dtype: int64"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Still, it's not really clear what the words are. We need to go further.\n",
      "\n",
      "- Let's use lowecase for standardisation.\n",
      "- Let's remove punctuations. Maybe any word that *even contains punctuation*, like \"I'm\" or \"&amp;\"\n",
      "- All single-letter words are a good idea to drop off too, like \"u\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "relevant_words = words.str.lower()\n",
      "relevant_words = relevant_words[~relevant_words.str.contains(r'[^a-z]')]\n",
      "relevant_words = relevant_words[relevant_words.str.len() > 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ignore = set(stopwords.words('english')) & set(relevant_words.unique())\n",
      "relevant_words.value_counts().drop(ignore)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 62,
       "text": [
        "good      543\n",
        "like      418\n",
        "hai       386\n",
        "one       365\n",
        "love      351\n",
        "time      321\n",
        "get       300\n",
        "new       298\n",
        "people    297\n",
        "see       273\n",
        "day       271\n",
        "ios       255\n",
        "rt        247\n",
        "ki        242\n",
        "ur        242\n",
        "...\n",
        "ghalib        1\n",
        "leya          1\n",
        "pudhcha       1\n",
        "pilgrim       1\n",
        "soiled        1\n",
        "lool          1\n",
        "krissh        1\n",
        "imo           1\n",
        "muaaaaah      1\n",
        "pranam        1\n",
        "bevkoof       1\n",
        "destroyed     1\n",
        "quater        1\n",
        "vasundhara    1\n",
        "validity      1\n",
        "Length: 19234, dtype: int64"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This list is a lot more meaningful.\n",
      "\n",
      "But before we go ahead, let's take a quick look at the *words we've ignored* to see if we should've taken something from there."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words.drop(relevant_words.index).str.lower().value_counts().head(30)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "                2441\n",
        "a               2377\n",
        "i               2161\n",
        "i'm              980\n",
        "u                778\n",
        "-                604\n",
        "@                507\n",
        ":)               493\n",
        "&amp;            467\n",
        "(@               363\n",
        "don't            292\n",
        "n                291\n",
        ":p               287\n",
        "r                285\n",
        "!                281\n",
        "it's             243\n",
        ":d               241\n",
        "7                240\n",
        "#ios7            232\n",
        "#forsale         227\n",
        "#flat            226\n",
        "2                217\n",
        ".                216\n",
        "?                215\n",
        "!!               204\n",
        "#residential     204\n",
        "..               196\n",
        ",                191\n",
        "#bappamorya      189\n",
        ":-)              173\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "... Ah! We're missing all the smileys (which may be OK) and the hashtags (which could be useful). Should we just pull in the hashtags alone? Let's do that. We'll allow `#` as an exception. We'll also ignore `@` which usually indicates reply to a person."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "relevant_words = words.str.lower()\n",
      "relevant_words = relevant_words[~relevant_words.str.contains(r'[^#@a-z]')]\n",
      "relevant_words = relevant_words[relevant_words.str.len() > 1]\n",
      "ignore = set(stopwords.words('english')) & set(relevant_words.unique())\n",
      "relevant_words.value_counts().drop(ignore)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 69,
       "text": [
        "good      543\n",
        "like      418\n",
        "hai       386\n",
        "one       365\n",
        "love      351\n",
        "time      321\n",
        "get       300\n",
        "new       298\n",
        "people    297\n",
        "see       273\n",
        "day       271\n",
        "ios       255\n",
        "rt        247\n",
        "ki        242\n",
        "ur        242\n",
        "...\n",
        "dekhaunchu      1\n",
        "jokingly        1\n",
        "inclination     1\n",
        "bd              1\n",
        "bf              1\n",
        "sants           1\n",
        "@itweetfacts    1\n",
        "dictated        1\n",
        "bk              1\n",
        "#instaholic     1\n",
        "jaaoege         1\n",
        "mahmood         1\n",
        "br              1\n",
        "#justbeingme    1\n",
        "betch           1\n",
        "Length: 25588, dtype: int64"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We haven't added anything to the list of top words, but further down, it may be useful.\n",
      "\n",
      "## See this as a word cloud\n",
      "\n",
      "Let's get the data into a DataFrame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top_words = relevant_words.value_counts().drop(ignore).reset_index()\n",
      "top_words.columns = ['word', 'count']\n",
      "top_words.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>word</th>\n",
        "      <th>count</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> good</td>\n",
        "      <td> 543</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> like</td>\n",
        "      <td> 418</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>  hai</td>\n",
        "      <td> 386</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>  one</td>\n",
        "      <td> 365</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> love</td>\n",
        "      <td> 351</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 76,
       "text": [
        "   word  count\n",
        "0  good    543\n",
        "1  like    418\n",
        "2   hai    386\n",
        "3   one    365\n",
        "4  love    351"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from vis import T, SVG\n",
      "from IPython.display import HTML"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    }
   ],
   "metadata": {}
  }
 ]
}